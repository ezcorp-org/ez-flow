# Story 3.2: GPU Acceleration - Metal (Apple Silicon)

## Status

Draft

## Story

**As a** user with an Apple Silicon Mac,
**I want** transcription to use the Neural Engine / GPU,
**so that** I get faster transcription on my M-series Mac.

## Acceptance Criteria

1. Detect Apple Silicon and Metal availability at runtime
2. `whisper-rs` compiled with Metal/CoreML support
3. Automatically uses Metal when available (no user action needed)
4. Works with all model sizes
5. Fallback to CPU for Intel Macs
6. Performance comparable to or better than CPU-only
7. No additional user configuration required

## Tasks / Subtasks

- [ ] **Task 1: Add Metal Feature Flag** (AC: 2)
  - [ ] Add `metal` feature to Cargo.toml
  - [ ] Conditionally compile whisper-rs with Metal
  - [ ] Only enable on macOS builds

- [ ] **Task 2: Implement Apple Silicon Detection** (AC: 1)
  - [ ] Detect Apple Silicon vs Intel CPU
  - [ ] Check Metal availability
  - [ ] Get device info for display

- [ ] **Task 3: Update Whisper Engine for Metal** (AC: 3)
  - [ ] Auto-enable Metal on Apple Silicon
  - [ ] Configure WhisperContextParameters for Metal
  - [ ] No user toggle needed (automatic)

- [ ] **Task 4: Implement Intel Fallback** (AC: 5)
  - [ ] Detect Intel Mac
  - [ ] Use CPU-only mode
  - [ ] Log hardware info

- [ ] **Task 5: Performance Verification** (AC: 6)
  - [ ] Benchmark Metal vs CPU on Apple Silicon
  - [ ] Ensure Metal is faster or equal
  - [ ] Log performance metrics

- [ ] **Task 6: Update CI for Metal Builds** (AC: 2)
  - [ ] Build universal binary (arm64 + x64)
  - [ ] Enable Metal feature for arm64
  - [ ] CPU-only for x64

- [ ] **Task 7: Add Tests**
  - [ ] Test Apple Silicon detection
  - [ ] Test Metal initialization
  - [ ] Test Intel fallback

## Dev Notes

### Feature Flag Configuration

```toml
# src-tauri/Cargo.toml
[features]
default = []
cuda = ["whisper-rs/cuda"]
metal = ["whisper-rs/metal"]

[target.'cfg(target_os = "macos")'.dependencies]
# Metal is built into macOS, no extra deps needed
```

### Apple Silicon Detection

```rust
// src-tauri/src/services/transcription/gpu.rs

#[cfg(target_os = "macos")]
pub fn detect_apple_silicon() -> bool {
    use std::process::Command;

    let output = Command::new("sysctl")
        .args(["-n", "machdep.cpu.brand_string"])
        .output()
        .ok();

    output
        .map(|o| String::from_utf8_lossy(&o.stdout).contains("Apple"))
        .unwrap_or(false)
}

#[cfg(target_os = "macos")]
pub fn detect_metal_device() -> Option<MetalDeviceInfo> {
    if detect_apple_silicon() {
        let output = std::process::Command::new("system_profiler")
            .args(["SPDisplaysDataType", "-json"])
            .output()
            .ok()?;

        // Parse GPU info from system_profiler
        Some(MetalDeviceInfo {
            name: "Apple Silicon GPU".into(),
            is_apple_silicon: true,
        })
    } else {
        None
    }
}

struct MetalDeviceInfo {
    name: String,
    is_apple_silicon: bool,
}
```

### Automatic Metal Initialization

```rust
impl WhisperEngine {
    pub fn new_with_auto_gpu() -> Self {
        let backend = detect_gpu_backend();

        tracing::info!("Detected GPU backend: {:?}", backend);

        Self {
            ctx: None,
            backend,
            model_id: String::new(),
        }
    }

    #[cfg(target_os = "macos")]
    pub fn load_model(&mut self, path: &Path) -> Result<(), ModelError> {
        let mut params = WhisperContextParameters::default();

        // Auto-enable Metal on Apple Silicon
        #[cfg(feature = "metal")]
        if matches!(self.backend, GpuBackend::Metal { .. }) {
            params = params.use_gpu(true);
            tracing::info!("Using Metal GPU acceleration");
        }

        let ctx = WhisperContext::new_with_params(path.to_str().unwrap(), params)?;
        self.ctx = Some(ctx);
        Ok(())
    }
}
```

### CI Configuration for Universal Binary

```yaml
# .github/workflows/release.yml
jobs:
  build-macos:
    runs-on: macos-latest
    strategy:
      matrix:
        include:
          - target: aarch64-apple-darwin
            features: metal
            artifact: macos-arm64
          - target: x86_64-apple-darwin
            features: ""
            artifact: macos-x64

    steps:
      - name: Add Rust target
        run: rustup target add ${{ matrix.target }}

      - name: Build
        run: |
          cargo tauri build \
            --target ${{ matrix.target }} \
            ${{ matrix.features && format('--features {0}', matrix.features) || '' }}
```

### Performance Expectations

| Backend | Tiny | Base | Small | Medium |
|---------|------|------|-------|--------|
| CPU (M1) | ~0.8x RT | ~1.2x RT | ~2.5x RT | ~5x RT |
| Metal (M1) | ~0.15x RT | ~0.2x RT | ~0.4x RT | ~0.8x RT |

Metal typically provides 4-6x speedup on Apple Silicon.

### File Locations

| File | Purpose |
|------|---------|
| `src-tauri/src/services/transcription/gpu.rs` | GPU detection (shared) |
| `src-tauri/Cargo.toml` | Metal feature flag |

### Testing

**Test Scenarios**:
- `test_apple_silicon_detection` - Correctly identifies M1/M2/M3
- `test_intel_mac_fallback` - Intel Macs use CPU
- `test_metal_transcription` - Metal produces correct results

**Manual Testing**:
- Test on M1, M2, M3 Macs
- Test on Intel Mac (should work, CPU only)
- Compare transcription times

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2024-12-28 | 0.1 | Initial story draft | Bob (SM) |

---

## Dev Agent Record

### Agent Model Used
*To be filled by dev agent*

### Debug Log References
*To be filled by dev agent*

### Completion Notes List
*To be filled by dev agent*

### File List
*To be filled by dev agent*

---

## QA Results
*To be filled by QA agent*
