# Story 3.1: GPU Acceleration - CUDA (NVIDIA)

## Status

Ready

## Story

**As a** user with an NVIDIA GPU,
**I want** transcription to use my GPU,
**so that** I get dramatically faster results (5-10x speedup).

## Acceptance Criteria

1. Detect NVIDIA GPU and CUDA availability at runtime
2. `whisper-rs` compiled with CUDA support (feature flag)
3. Settings toggle: "Use GPU acceleration" (auto-detected default)
4. GPU transcription works with all model sizes
5. Graceful fallback to CPU if CUDA initialization fails
6. GPU memory usage displayed in Settings/About
7. Performance metrics: show transcription time in history (GPU vs CPU comparison)
8. Windows and Linux support (CUDA not available on macOS)

## Tasks / Subtasks

- [ ] **Task 1: Add CUDA Feature Flag** (AC: 2)
  - [ ] Add `cuda` feature to Cargo.toml
  - [ ] Conditionally compile whisper-rs with CUDA
  - [ ] Update CI to build CUDA variant

- [ ] **Task 2: Implement GPU Detection** (AC: 1)
  - [ ] Create `src-tauri/src/services/transcription/gpu.rs`
  - [ ] Detect NVIDIA GPU presence
  - [ ] Check CUDA runtime availability
  - [ ] Get GPU info (name, VRAM)

- [ ] **Task 3: Update Whisper Engine for GPU** (AC: 4)
  - [ ] Modify model loading to use GPU context
  - [ ] Pass GPU flag to WhisperContextParameters
  - [ ] Handle GPU memory allocation

- [ ] **Task 4: Implement Fallback Logic** (AC: 5)
  - [ ] Try GPU initialization first
  - [ ] Catch initialization errors
  - [ ] Fall back to CPU silently
  - [ ] Log fallback with tracing

- [ ] **Task 5: Add GPU Toggle in Settings** (AC: 3)
  - [ ] Add `use_gpu` to Settings
  - [ ] Auto-detect and set default
  - [ ] Allow manual override
  - [ ] Reload model when toggled

- [ ] **Task 6: Display GPU Info** (AC: 6)
  - [ ] Show GPU name in Settings/About
  - [ ] Display VRAM total/used
  - [ ] Update during transcription

- [ ] **Task 7: Track Performance Metrics** (AC: 7)
  - [ ] Add `gpu_used` flag to TranscriptionResult
  - [ ] Display in history entries
  - [ ] Compare CPU vs GPU times

- [ ] **Task 8: Update CI/CD for CUDA Builds** (AC: 8)
  - [ ] Add CUDA toolkit to Windows CI
  - [ ] Add CUDA toolkit to Linux CI
  - [ ] Create separate CUDA artifacts
  - [ ] Skip CUDA build on macOS

- [ ] **Task 9: Add Unit Tests**
  - [ ] Test GPU detection
  - [ ] Test fallback logic
  - [ ] Test feature flag compilation

## Dev Notes

### Feature Flag Configuration [Source: architecture.md#9.1]

```toml
# src-tauri/Cargo.toml
[features]
default = []
cuda = ["whisper-rs/cuda"]
```

### GPU Detection [Source: architecture.md#9.2]

```rust
// src-tauri/src/services/transcription/gpu.rs

#[derive(Debug, Clone, Serialize)]
pub enum GpuBackend {
    Cuda { device_name: String, vram_mb: u64 },
    Cpu,
}

pub fn detect_gpu_backend() -> GpuBackend {
    #[cfg(feature = "cuda")]
    {
        if let Some(info) = detect_cuda_device() {
            return GpuBackend::Cuda {
                device_name: info.name,
                vram_mb: info.vram_mb,
            };
        }
    }

    GpuBackend::Cpu
}

#[cfg(feature = "cuda")]
fn detect_cuda_device() -> Option<CudaDeviceInfo> {
    // Use nvml-wrapper or cuda-sys to detect device
    use nvml_wrapper::Nvml;

    let nvml = Nvml::init().ok()?;
    let device = nvml.device_by_index(0).ok()?;
    let name = device.name().ok()?;
    let memory = device.memory_info().ok()?;

    Some(CudaDeviceInfo {
        name,
        vram_mb: memory.total / 1024 / 1024,
    })
}

struct CudaDeviceInfo {
    name: String,
    vram_mb: u64,
}
```

### Model Loading with GPU [Source: architecture.md#9.3]

```rust
impl WhisperEngine {
    pub fn load_model_with_gpu(
        &mut self,
        path: &Path,
        use_gpu: bool,
    ) -> Result<(), ModelError> {
        let mut params = WhisperContextParameters::default();

        #[cfg(feature = "cuda")]
        if use_gpu {
            params = params.use_gpu(true);
            tracing::info!("Loading model with CUDA GPU acceleration");
        }

        match WhisperContext::new_with_params(path.to_str().unwrap(), params) {
            Ok(ctx) => {
                self.ctx = Some(ctx);
                self.using_gpu = use_gpu;
                Ok(())
            }
            Err(e) if use_gpu => {
                tracing::warn!("GPU initialization failed, falling back to CPU: {}", e);
                self.load_model_with_gpu(path, false)
            }
            Err(e) => Err(ModelError::LoadFailed(e.to_string())),
        }
    }
}
```

### CI Configuration for CUDA

```yaml
# .github/workflows/release.yml
jobs:
  build-cuda:
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        include:
          - os: windows-latest
            target: x86_64-pc-windows-msvc
          - os: ubuntu-22.04
            target: x86_64-unknown-linux-gnu

    steps:
      - uses: Jimver/cuda-toolkit@v0.2.11
        with:
          cuda: '12.2.0'

      - name: Build with CUDA
        run: cargo tauri build --features cuda
```

### Settings Model Update

```rust
// src-tauri/src/models/settings.rs
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Settings {
    // ... existing fields

    #[serde(default = "default_use_gpu")]
    pub use_gpu: bool,
}

fn default_use_gpu() -> bool {
    // Auto-detect GPU availability
    matches!(detect_gpu_backend(), GpuBackend::Cuda { .. })
}
```

### Performance Characteristics [Source: architecture.md#9.4]

| Backend | Tiny | Base | Small | Medium |
|---------|------|------|-------|--------|
| CPU (AVX2) | ~1.0x RT | ~1.5x RT | ~3x RT | ~6x RT |
| CUDA (RTX 3060) | ~0.1x RT | ~0.15x RT | ~0.3x RT | ~0.6x RT |

*RT = Real-time factor*

### File Locations

| File | Purpose |
|------|---------|
| `src-tauri/src/services/transcription/gpu.rs` | GPU detection |
| `src-tauri/Cargo.toml` | CUDA feature flag |
| `.github/workflows/release.yml` | CUDA CI builds |

### Testing

**Test Scenarios**:
- `test_gpu_detection` - Detect GPU when available
- `test_fallback_to_cpu` - Fallback works when GPU fails
- `test_transcription_with_gpu` - GPU transcription produces correct results

**Manual Testing**:
- Verify CUDA builds work on Windows/Linux
- Test with different NVIDIA GPUs
- Measure speedup vs CPU

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2024-12-28 | 0.1 | Initial story draft | Bob (SM) |

---

## Dev Agent Record

### Agent Model Used
*To be filled by dev agent*

### Debug Log References
*To be filled by dev agent*

### Completion Notes List
*To be filled by dev agent*

### File List
*To be filled by dev agent*

---

## QA Results
*To be filled by QA agent*
