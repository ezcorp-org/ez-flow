# Story 3.9: Performance Optimization & Telemetry

## Status

Ready

## Story

**As a** user,
**I want** EZ Flow to be fast and lightweight,
**so that** it doesn't slow down my computer or drain battery.

## Acceptance Criteria

1. Idle memory usage < 50MB (model not loaded)
2. Model lazy-loaded on first transcription (not at startup)
3. Model unloaded after configurable idle timeout (saves memory)
4. Startup time < 3 seconds to tray icon ready
5. CPU usage near 0% when idle
6. Optional anonymous usage telemetry (opt-in, disabled by default)
7. Performance benchmarks documented in README

## Tasks / Subtasks

- [ ] **Task 1: Implement Lazy Model Loading** (AC: 2)
  - [ ] Don't load model on app start
  - [ ] Load on first transcription request
  - [ ] Show loading indicator during first load
  - [ ] Cache loaded model in memory

- [ ] **Task 2: Implement Model Unloading** (AC: 3)
  - [ ] Track last transcription time
  - [ ] Unload after configurable timeout (default: 5 min)
  - [ ] Add setting for idle timeout
  - [ ] Reload when needed

- [ ] **Task 3: Optimize Startup Time** (AC: 4)
  - [ ] Profile startup sequence
  - [ ] Defer non-essential initialization
  - [ ] Optimize asset loading
  - [ ] Measure and log startup time

- [ ] **Task 4: Reduce Idle Memory** (AC: 1, 5)
  - [ ] Profile memory usage
  - [ ] Identify and fix memory leaks
  - [ ] Reduce frontend bundle size
  - [ ] Use lazy loading for UI components

- [ ] **Task 5: Minimize CPU Usage** (AC: 5)
  - [ ] Remove unnecessary timers/polling
  - [ ] Use event-driven architecture
  - [ ] Profile CPU in idle state
  - [ ] Optimize audio monitoring

- [ ] **Task 6: Implement Telemetry (Opt-in)** (AC: 6)
  - [ ] Design telemetry data structure
  - [ ] Implement opt-in setting
  - [ ] Add telemetry endpoint
  - [ ] Only collect anonymous usage stats

- [ ] **Task 7: Create Performance Benchmarks** (AC: 7)
  - [ ] Benchmark startup time
  - [ ] Benchmark memory usage
  - [ ] Benchmark transcription speed
  - [ ] Document in README

- [ ] **Task 8: Add Performance Monitoring**
  - [ ] Add tracing for performance metrics
  - [ ] Log slow operations
  - [ ] Create performance dashboard (optional)

- [ ] **Task 9: Add Tests**
  - [ ] Test lazy loading works
  - [ ] Test model unloading
  - [ ] Test memory stays under limit

## Dev Notes

### Lazy Model Loading

```rust
// src-tauri/src/services/transcription/engine.rs

pub struct WhisperEngine {
    ctx: Option<WhisperContext>,
    model_path: PathBuf,
    last_used: Arc<RwLock<Instant>>,
    loading: Arc<AtomicBool>,
}

impl WhisperEngine {
    pub fn new(model_path: PathBuf) -> Self {
        Self {
            ctx: None,
            model_path,
            last_used: Arc::new(RwLock::new(Instant::now())),
            loading: Arc::new(AtomicBool::new(false)),
        }
    }

    pub async fn ensure_loaded(&mut self) -> Result<(), ModelError> {
        if self.ctx.is_some() {
            *self.last_used.write().await = Instant::now();
            return Ok(());
        }

        if self.loading.swap(true, Ordering::SeqCst) {
            // Already loading, wait
            while self.loading.load(Ordering::SeqCst) {
                tokio::time::sleep(Duration::from_millis(100)).await;
            }
            return Ok(());
        }

        tracing::info!("Lazy loading Whisper model...");
        let start = Instant::now();

        let params = WhisperContextParameters::default();
        let ctx = WhisperContext::new_with_params(
            self.model_path.to_str().unwrap(),
            params
        )?;

        self.ctx = Some(ctx);
        self.loading.store(false, Ordering::SeqCst);
        *self.last_used.write().await = Instant::now();

        tracing::info!("Model loaded in {:?}", start.elapsed());
        Ok(())
    }

    pub async fn transcribe(&mut self, audio: &[f32]) -> Result<TranscriptionResult, TranscriptionError> {
        self.ensure_loaded().await?;
        // ... transcription logic
    }
}
```

### Model Unloading

```rust
// src-tauri/src/services/transcription/manager.rs

pub struct ModelManager {
    engine: Arc<RwLock<WhisperEngine>>,
    idle_timeout: Duration,
}

impl ModelManager {
    pub fn start_idle_monitor(self: Arc<Self>) {
        tokio::spawn(async move {
            loop {
                tokio::time::sleep(Duration::from_secs(60)).await;

                let engine = self.engine.read().await;
                let last_used = *engine.last_used.read().await;

                if last_used.elapsed() > self.idle_timeout && engine.ctx.is_some() {
                    drop(engine);
                    let mut engine = self.engine.write().await;
                    tracing::info!("Unloading model due to idle timeout");
                    engine.ctx = None;
                }
            }
        });
    }
}
```

### Startup Optimization

```rust
// src-tauri/src/main.rs

fn main() {
    let start = Instant::now();

    tauri::Builder::default()
        .setup(|app| {
            // Measure setup phases
            let setup_start = Instant::now();

            // Essential: Tray icon (fast)
            setup_tray(app)?;
            tracing::debug!("Tray setup: {:?}", setup_start.elapsed());

            // Deferred: Settings, DB, etc.
            let state_start = Instant::now();
            let state = AppState::new_lazy()?;
            app.manage(state);
            tracing::debug!("State setup: {:?}", state_start.elapsed());

            tracing::info!("Startup complete in {:?}", start.elapsed());
            Ok(())
        })
        .run(tauri::generate_context!())
        .expect("error running app");
}
```

### Telemetry Data Structure

```rust
// src-tauri/src/services/telemetry.rs

#[derive(Debug, Serialize)]
pub struct TelemetryEvent {
    pub event_type: EventType,
    pub timestamp: String,
    pub app_version: String,
    pub os: String,
    pub arch: String,
    // Anonymous - no user identifiers
}

#[derive(Debug, Serialize)]
pub enum EventType {
    AppLaunched,
    TranscriptionCompleted {
        duration_secs: f32,
        model_id: String,
        gpu_used: bool,
    },
    ModelDownloaded {
        model_id: String,
    },
}

pub struct TelemetryService {
    enabled: bool,
    endpoint: String,
}

impl TelemetryService {
    pub async fn track(&self, event: TelemetryEvent) {
        if !self.enabled {
            return;
        }

        // Fire and forget
        let _ = reqwest::Client::new()
            .post(&self.endpoint)
            .json(&event)
            .send()
            .await;
    }
}
```

### Performance Benchmarks

```markdown
## Performance Benchmarks

Tested on recommended hardware (M1 Mac / i7-10700 / Ryzen 5600X).

### Startup Time
| Metric | Target | Measured |
|--------|--------|----------|
| To tray icon | < 3s | 1.2s |
| First transcription ready | < 5s | 3.5s |

### Memory Usage
| State | Target | Measured |
|-------|--------|----------|
| Idle (no model) | < 50 MB | 35 MB |
| Model loaded (base) | < 500 MB | 420 MB |
| During transcription | < 800 MB | 650 MB |

### CPU Usage
| State | Target | Measured |
|-------|--------|----------|
| Idle | < 1% | 0.1% |
| Recording | < 5% | 2% |
| Transcribing | N/A (full usage ok) | 80-100% |

### Transcription Speed (Base Model)
| Audio Duration | CPU | GPU (Metal/CUDA) |
|----------------|-----|------------------|
| 5 seconds | 1.5x RT | 0.2x RT |
| 30 seconds | 1.5x RT | 0.2x RT |
| 5 minutes | 1.5x RT | 0.2x RT |

*RT = Real-time (1.0x = audio duration equals processing time)*
```

### File Locations

| File | Purpose |
|------|---------|
| `src-tauri/src/services/transcription/manager.rs` | Model lifecycle |
| `src-tauri/src/services/telemetry.rs` | Telemetry service |
| `docs/performance.md` | Benchmark documentation |

### Testing

**Performance Tests**:
```rust
#[tokio::test]
async fn test_idle_memory_under_50mb() {
    let app = start_app_headless().await;
    let memory = get_process_memory();
    assert!(memory < 50 * 1024 * 1024, "Idle memory {} exceeds 50MB", memory);
}

#[tokio::test]
async fn test_startup_under_3s() {
    let start = Instant::now();
    let app = start_app_headless().await;
    assert!(start.elapsed() < Duration::from_secs(3));
}

#[tokio::test]
async fn test_model_unloads_after_timeout() {
    let engine = WhisperEngine::new(model_path);
    engine.ensure_loaded().await.unwrap();
    assert!(engine.ctx.is_some());

    // Simulate timeout
    tokio::time::sleep(IDLE_TIMEOUT + Duration::from_secs(1)).await;
    engine.check_idle_unload().await;

    assert!(engine.ctx.is_none());
}
```

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2024-12-28 | 0.1 | Initial story draft | Bob (SM) |

---

## Dev Agent Record

### Agent Model Used
*To be filled by dev agent*

### Debug Log References
*To be filled by dev agent*

### Completion Notes List
*To be filled by dev agent*

### File List
*To be filled by dev agent*

---

## QA Results
*To be filled by QA agent*
